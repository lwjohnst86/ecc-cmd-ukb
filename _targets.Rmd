---
title: "Target Markdown"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

Target Markdown is a powerful R Markdown interface for reproducible analysis
pipelines, and the chapter at https://books.ropensci.org/targets/markdown.html
walks through it in detail. 

# Packages

The example requires several R packages that can be installed using the remotes package:

```{r install-dependencies, eval=FALSE}
install.packages("remotes")
remotes::install_deps()
```

# Setup

If you are using old versions of `targets` (<= 0.7.0) and/or `knitr` (<= 1.33),
you will need to load the `targets` package in the R Markdown document in order
for Target Markdown code chunks to work.

Near the top of the document, you may also wish to remove the `_targets_r`
directory previously written by non-interactive runs of the report. Otherwise,
your pipeline may contain superfluous targets.

```{r}
library(targets)
tar_unscript()

# Use this function to delete target's entire cache. Is a hard reset.
# tar_destroy()
```

# Globals

We first define some global options/functions common to all targets.

```{targets example-globals, tar_globals = TRUE}
options(tidyverse.quiet = TRUE)
library(tidyverse)
pkgs_to_load <- desc::desc_get_deps() %>% 
    filter(type == "Imports") %>% 
    pull(package)
# Get and set packages that pipeline depends on.
tar_option_set(packages = pkgs_to_load)
here::here("R") %>% 
    fs::dir_ls(glob = "*.R") %>% 
    walk(source)
```

# Targets

Initial processing of the UK Biobank to get into the necessary state needed for
this project. The end goal is to save the dataset at the storage location.

```{r, eval=FALSE}
# Use to copy variable list for the function to wrangle and pre-prepare the UK
# Biobank dataset.:
# column_specs <- initial_import_to_copy_variable_specs()

# Run this to pre-prepare the datasets:
# ukb_data_raw <- ukb_import_with_specific_columns()

# But better to run as a job first:
# run_job_importing_raw()

# Then run the wrangling function:
# wrangled_data <- ukb_wrangle_and_save(ukb_data_raw)

# But really, this function is used to save the dataset to its location:
# ukb_wrangle_and_save(ukb_data_raw, .save = TRUE)
```

Next we need to process the dataset for exact use in this project and load it into
the environment. We don't want to save this dataset, since it is already in its
remote location.

```{targets data_file_path}
tar_target(
    name = data_file_path,
    command = check_if_data_exists(),
    format = "file"
)
```

Let's import the data from the storage location, without removing exclusions.

```{targets project-data-pre-exclusions}
tar_target(
    name = project_data_pre_exclusions, 
    command = ukb_import_project_data(data_file_path)
)
```

Since we want to create a CONSORT diagram of who we removed from the original
UK Biobank dataset, we send the consort data into the plot function.

```{targets consort-diagram}
tar_target(
    name = consort_diagram,
    command = project_data_pre_exclusions %>%
        ukb_remove_exclusions(for_consort_diagram = TRUE) %>%
        plot_consort_diagram(save_plot = TRUE),
    format = "file"
)
```

Now we can remove participants based on the exclusion criteria and other removals
that we found during exploration.

```{targets project-data}
tar_target(
    name = project_data,
    command = ukb_remove_exclusions(project_data_pre_exclusions)
)
```


```{r}
library(tidyverse)
library(skimr)

# project_data <- prepare_data_for_netcoupler_analysis(check_if_data_exists())
```

We also want to create some cross-validation splits to eventually check the
results against.

```{targets project-data-cv}
tar_target(
    name = project_data_cv, 
    command = project_data %>% 
        rsample::training() %>% 
        create_cv_splits()
)
```

Next is to set up the 

TODO:

- Descriptive statistics, by sex/gender (including sample size) (skimr?)
- Update confounders based on Christina's paper
    - Smoking? Physical activity? Ethnic background, education, income, anthropometrics?
    - Make DAG.
- Plot of all metabolic variables
- Network results by sex/gender
- (In manuscript Rmd instead of targets?) Plot of network results by sex/gender
as well as total.
- Exposure and outcome link results by sex/gender
- (In manuscript Rmd instead of targets?) Plot of linkage results by sex/gender
as well as total.
- List of variables and their meta data (from UK Biobank)

```{targets network-results}
tar_targets(
    name = network_results,
    # Generate network results as rda file in data/
    command = generate_network_results(project_data_cv$splits),
    format = "file"
)
```

And eventually make the manuscript:

```{r}
# tar_render(report, "report.Rmd")
```

# Pipeline

If you ran all the `{targets}` chunks in non-interactive mode, then your R scripts are set up to run the pipeline.

```{r}
tar_make()
# Don't need or want the project data in the target cache.
tar_delete(starts_with("project_data"))
```

# Output

You can retrieve results from the `_targets/` data store using `tar_read()` or `tar_load()`.

```{r, message = FALSE}
library(biglm)
tar_read(fit)
```

```{r}
tar_read(hist)
```

The `targets` dependency graph helps your readers understand the steps of your pipeline at a high level.

```{r}
tar_visnetwork()
```

At this point, you can go back and run `{targets}` chunks in interactive mode without interfering with the code or data of the non-interactive pipeline.
